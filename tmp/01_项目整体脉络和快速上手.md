# DeerFlow 项目整体脉络和快速上手指南

## 一、项目概述

**DeerFlow** (Deep Exploration and Efficient Research Flow) 是字节跳动开源的**多智能体深度研究框架**，用于自动化进行深度研究、生成报告、创建播客和演示文稿。

### 核心定位
- **技术栈**: Python 3.12+ 后端 + Next.js 15+ 前端
- **核心框架**: 基于 LangGraph 构建的多智能体协作系统
- **应用场景**: 深度研究、报告生成、内容创作、知识管理

### 项目亮点
- ✅ 多智能体协作架构（Coordinator、Planner、Researcher、Coder、Reporter）
- ✅ 支持多种 LLM 提供商（OpenAI、DeepSeek、Google、Azure、Qwen 等）
- ✅ 集成 MCP（Model Context Protocol）服务
- ✅ RAG（检索增强生成）支持多种向量数据库
- ✅ 丰富的工具集成（搜索、爬虫、代码执行、TTS）
- ✅ 人机协作机制（多轮澄清、计划审核）

---

## 二、项目架构脉络

### 2.1 整体架构图

```
用户输入
    ↓
Coordinator (协调器)
    ↓
Background Investigator (背景调查)
    ↓
Planner (规划器) ←→ 人工审核反馈
    ↓
Research Team (研究团队)
    ├→ Researcher (研究员) - 网络搜索、爬虫
    ├→ Analyst (分析员) - 信息综合
    └→ Coder (编码员) - 数据处理、代码执行
    ↓
Reporter (报告生成器)
    ↓
最终报告 + 引用 + 资源
```

### 2.2 核心模块说明

| 模块 | 路径 | 职责 |
|------|------|------|
| **智能体系统** | `src/agents/` | 定义和配置各类智能体 |
| **工作流引擎** | `src/graph/` | LangGraph 工作流构建和状态管理 |
| **LLM 集成** | `src/llms/` | 多 LLM 提供商统一接口 |
| **工具集** | `src/tools/` | 搜索、爬虫、代码执行、RAG 等工具 |
| **提示词库** | `src/prompts/` | 各智能体的提示词模板 |
| **Web 服务** | `src/server/` | FastAPI 后端 API |
| **前端界面** | `web/` | Next.js + React 19 前端 |
| **RAG 系统** | `src/rag/` | 向量数据库集成（Milvus、Qdrant） |
| **内容生成** | `src/podcast/`, `src/ppt/`, `src/prose/` | 播客、PPT、散文生成工作流 |

### 2.3 技术栈清单

**后端核心依赖：**
- `langchain` + `langgraph` - LLM 应用框架和工作流引擎
- `fastapi` + `uvicorn` - Web 服务框架
- `litellm` - 统一多 LLM 提供商接口
- `pymilvus` / `qdrant-client` - 向量数据库客户端
- `mcp` - Model Context Protocol 集成

**前端技术栈：**
- Next.js 15 + React 19 + TypeScript
- Tailwind CSS + Shadcn/ui
- TipTap - 富文本编辑器
- Mermaid - 图表绘制

---

## 三、快速上手指南

### 3.1 环境准备

**系统要求：**
- Python 3.12+
- Node.js 22+
- （推荐）uv - Python 环境管理
- （推荐）pnpm - Node.js 包管理

**安装步骤：**

```bash
# 1. 克隆项目
git clone https://github.com/bytedance/deer-flow.git
cd deer-flow

# 2. 安装 Python 依赖（uv 会自动创建虚拟环境）
uv sync

# 3. 配置环境变量
cp .env.example .env
# 编辑 .env，填入 API 密钥：
# - TAVILY_API_KEY 或 INFOQUEST_API_KEY（搜索引擎）
# - 其他可选：TTS、RAG 等

# 4. 配置 LLM 模型
cp conf.yaml.example conf.yaml
# 编辑 conf.yaml，配置你的 LLM 提供商和模型

# 5. 安装前端依赖（可选，如果需要 Web UI）
cd web
pnpm install
cd ..
```

### 3.2 运行方式

**方式一：命令行模式（最快）**

```bash
# 直接运行
uv run main.py

# 或指定研究主题
uv run main.py "What is quantum computing?"

# 交互式模式（内置问题列表）
uv run main.py --interactive
```

**方式二：Web UI 模式（推荐）**

```bash
# 启动后端和前端（开发模式）
./bootstrap.sh -d    # macOS/Linux
bootstrap.bat -d     # Windows

# 访问 http://localhost:3000
```

**方式三：Docker 部署**

```bash
# 构建镜像
docker compose build

# 启动服务
docker compose up

# 访问 http://localhost:3000
```

### 3.3 配置要点

**必须配置：**

1. **搜索引擎 API**（`.env`）
   ```bash
   SEARCH_API=tavily  # 或 infoquest, duckduckgo, brave_search
   TAVILY_API_KEY=your_key_here
   ```

2. **LLM 模型**（`conf.yaml`）
   ```yaml
   LLM_MODELS:
     basic:
       provider: openai
       model: gpt-4o
       api_key: your_openai_key
   ```

**可选配置：**
- RAG 向量数据库（Milvus、Qdrant）
- MCP 服务器集成
- TTS 语音合成
- 检查点持久化（MongoDB、PostgreSQL）

---

## 四、核心功能详解

### 4.1 深度研究工作流 - 完整执行流程

#### 4.1.1 流程概览

```
用户输入 (main.py / Web API)
    ↓
[1] 初始化阶段 (Coordinator)
    ├─ 解析用户输入
    ├─ 初始化 LangGraph 状态
    └─ 检查是否需要澄清
    ↓
[2] 澄清阶段 (可选)
    ├─ 检测主题模糊度
    ├─ 生成澄清问题
    └─ 多轮对话收集信息
    ↓
[3] 背景调查阶段 (Background Investigator)
    ├─ 初步网络搜索 (Tavily/InfoQuest)
    ├─ 提取关键信息
    └─ 构建上下文知识
    ↓
[4] 计划生成阶段 (Planner)
    ├─ 分析研究主题和背景
    ├─ 生成研究步骤列表
    ├─ 分配智能体类型 (researcher/coder/analyst)
    └─ 估算资源需求
    ↓
[5] 人工审核阶段 (可选)
    ├─ 展示计划给用户
    ├─ 等待用户反馈
    └─ 根据反馈修改计划
    ↓
[6] 研究执行阶段 (Research Team)
    ├─ 并行执行研究步骤
    ├─ Researcher: 搜索 + 爬虫 + RAG
    ├─ Coder: 数据处理 + 可视化
    ├─ Analyst: 信息综合 + 推理
    └─ 收集所有结果到共享状态
    ↓
[7] 报告生成阶段 (Reporter)
    ├─ 整合所有研究结果
    ├─ 生成结构化报告
    ├─ 添加引用和资源链接
    └─ 格式化输出 (Markdown)
    ↓
[8] 输出阶段
    ├─ 保存报告文件
    ├─ 返回给用户 (CLI/Web)
    └─ 可选：生成播客/PPT
```

#### 4.1.2 详细流程说明

**阶段 1: 初始化 (Coordinator)**

*涉及文件：*
- `src/graph/builder.py` - 构建 LangGraph 工作流
- `src/graph/types.py` - 定义状态类型
- `src/agents/agents.py` - 智能体配置

*执行逻辑：*
```python
# 1. 创建初始状态
state = {
    "query": user_input,
    "messages": [],
    "research_plan": None,
    "research_results": [],
    "final_report": None
}

# 2. 启动 LangGraph 工作流
graph = create_research_graph()
result = graph.invoke(state)
```

*关键决策点：*
- 检查 `enable_clarification` 配置
- 判断是否需要进入澄清流程
- 初始化检查点（如果启用持久化）

---

**阶段 2: 澄清机制 (Clarification)**

*涉及文件：*
- `src/graph/nodes.py` - `clarification_node()`
- `src/prompts/clarification.py` - 澄清提示词

*执行逻辑：*
```python
# 1. 分析查询模糊度
ambiguity_score = analyze_query_clarity(query)

# 2. 如果模糊，生成澄清问题
if ambiguity_score > threshold:
    questions = generate_clarification_questions(query)

# 3. 多轮对话
for round in range(max_clarification_rounds):
    user_response = await get_user_input(questions)
    query = refine_query(query, user_response)
    if is_clear_enough(query):
        break
```

*状态更新：*
- `state["query"]` - 更新为澄清后的查询
- `state["messages"]` - 记录对话历史
- `state["clarification_done"]` - 标记完成

---

**阶段 3: 背景调查 (Background Investigation)**

*涉及文件：*
- `src/graph/nodes.py` - `background_investigation_node()`
- `src/tools/search.py` - 搜索工具
- `src/agents/agents.py` - Researcher 智能体

*执行逻辑：*
```python
# 1. 执行初步搜索
search_results = search_tool.invoke({
    "query": state["query"],
    "max_results": 5
})

# 2. 提取关键信息
background_info = extract_key_information(search_results)

# 3. 构建上下文
context = build_context(background_info)
```

*工具调用：*
- Tavily Search / InfoQuest
- 网页爬虫 (Jina Reader)
- RAG 检索（如果配置）

*状态更新：*
- `state["background_info"]` - 背景信息
- `state["search_results"]` - 搜索结果
- `state["context"]` - 上下文知识

---

**阶段 4: 计划生成 (Planning)**

*涉及文件：*
- `src/graph/nodes.py` - `planning_node()`
- `src/prompts/planner.py` - 规划提示词
- `src/agents/agents.py` - Planner 智能体

*执行逻辑：*
```python
# 1. 调用 Planner LLM
plan = planner_llm.invoke({
    "query": state["query"],
    "background": state["background_info"],
    "max_steps": config.max_step_num
})

# 2. 解析计划结构
research_plan = parse_plan(plan)
# 结构：[
#   {"step": 1, "description": "...", "agent": "researcher"},
#   {"step": 2, "description": "...", "agent": "coder"},
#   ...
# ]

# 3. 验证计划合理性
validated_plan = validate_plan(research_plan)
```

*计划格式：*
```json
{
  "steps": [
    {
      "step_id": 1,
      "description": "搜索量子计算的基本原理",
      "agent_type": "researcher",
      "tools": ["search", "crawler"],
      "expected_output": "量子计算基础知识"
    },
    {
      "step_id": 2,
      "description": "分析量子比特的数学模型",
      "agent_type": "coder",
      "tools": ["python_repl"],
      "expected_output": "数学公式和可视化"
    }
  ],
  "estimated_time": "5-10 minutes",
  "resource_requirements": ["search_api", "llm_calls"]
}
```

*状态更新：*
- `state["research_plan"]` - 完整计划
- `state["plan_approved"]` - 审核状态

---

**阶段 5: 人工审核 (Human Review)**

*涉及文件：*
- `src/graph/nodes.py` - `human_review_node()`
- `src/server/chat_stream.py` - Web 交互

*执行逻辑：*
```python
# 1. 展示计划
display_plan(state["research_plan"])

# 2. 等待用户反馈
if config.require_approval:
    feedback = await wait_for_user_feedback()

    # 3. 处理反馈
    if feedback.action == "approve":
        state["plan_approved"] = True
    elif feedback.action == "modify":
        state["research_plan"] = modify_plan(
            state["research_plan"],
            feedback.modifications
        )
    elif feedback.action == "reject":
        return "END"  # 终止流程
```

*交互方式：*
- CLI: 命令行输入
- Web: 实时聊天界面
- API: HTTP 回调

---

**阶段 6: 研究执行 (Research Execution)**

*涉及文件：*
- `src/graph/nodes.py` - `research_node()`
- `src/agents/agents.py` - Researcher, Coder, Analyst
- `src/tools/` - 各类工具实现

*执行逻辑：*
```python
# 1. 遍历计划步骤
for step in state["research_plan"]["steps"]:
    # 2. 根据智能体类型路由
    agent = select_agent(step["agent_type"])

    # 3. 执行步骤
    result = agent.invoke({
        "task": step["description"],
        "context": state["context"],
        "tools": step["tools"]
    })

    # 4. 收集结果
    state["research_results"].append({
        "step_id": step["step_id"],
        "result": result,
        "sources": result.get("sources", [])
    })
```

*智能体工具映射：*

| 智能体 | 可用工具 | 典型任务示例 |
|--------|----------|--------------|
| **Researcher** | `search`, `crawler`, `rag_query` | "搜索最新的 AI 论文" |
| **Coder** | `python_repl`, `data_analysis` | "分析数据集并生成图表" |
| **Analyst** | 无特定工具（纯推理） | "综合前面的信息得出结论" |

*并行执行优化：*
```python
# 如果步骤之间无依赖，可并行执行
independent_steps = identify_independent_steps(plan)
results = await asyncio.gather(*[
    execute_step(step) for step in independent_steps
])
```

*状态更新：*
- `state["research_results"]` - 所有步骤结果
- `state["sources"]` - 引用来源列表
- `state["artifacts"]` - 生成的文件（图表、代码等）

---

**阶段 7: 报告生成 (Report Generation)**

*涉及文件：*
- `src/graph/nodes.py` - `reporting_node()`
- `src/prompts/reporter.py` - 报告提示词
- `src/agents/agents.py` - Reporter 智能体

*执行逻辑：*
```python
# 1. 整合所有研究结果
consolidated_data = consolidate_results(
    state["research_results"]
)

# 2. 调用 Reporter LLM
report = reporter_llm.invoke({
    "query": state["query"],
    "research_data": consolidated_data,
    "sources": state["sources"],
    "format": "markdown"
})

# 3. 格式化报告
formatted_report = format_report(report, {
    "add_toc": True,
    "add_citations": True,
    "add_metadata": True
})
```

*报告结构：*
```markdown
# [研究主题]

## 摘要
[执行摘要]

## 目录
- 第一部分
- 第二部分
...

## 正文
### 第一部分
[内容]

### 第二部分
[内容]

## 结论
[总结]

## 参考资料
1. [来源1](url1)
2. [来源2](url2)

## 附录
- 生成的图表
- 数据文件
```

*状态更新：*
- `state["final_report"]` - 完整报告
- `state["report_metadata"]` - 元数据（字数、引用数等）

---

**阶段 8: 输出和后处理**

*涉及文件：*
- `main.py` - CLI 输出
- `src/server/chat_stream.py` - Web 流式输出
- `src/podcast/workflow.py` - 播客生成（可选）
- `src/ppt/workflow.py` - PPT 生成（可选）

*执行逻辑：*
```python
# 1. 保存报告
save_report(
    state["final_report"],
    filename=f"report_{timestamp}.md"
)

# 2. 返回给用户
if mode == "cli":
    print(state["final_report"])
elif mode == "web":
    stream_response(state["final_report"])
elif mode == "api":
    return JSONResponse(state)

# 3. 可选：生成其他格式
if config.generate_podcast:
    podcast = generate_podcast(state["final_report"])

if config.generate_ppt:
    ppt = generate_presentation(state["final_report"])
```

*输出内容：*
- Markdown 报告文件
- 引用来源列表 (JSON)
- 生成的资源文件（图表、数据）
- 可选：音频文件（播客）
- 可选：PPT 文件

---

#### 4.1.3 状态管理机制

**LangGraph 状态定义：**

```python
# src/graph/types.py
class ResearchState(TypedDict):
    # 输入
    query: str                    # 用户查询
    messages: List[BaseMessage]   # 对话历史

    # 中间状态
    clarification_done: bool      # 澄清完成标志
    background_info: str          # 背景信息
    research_plan: Dict           # 研究计划
    plan_approved: bool           # 计划审核状态

    # 研究结果
    research_results: List[Dict]  # 各步骤结果
    sources: List[str]            # 引用来源
    artifacts: List[str]          # 生成的文件

    # 输出
    final_report: str             # 最终报告
    report_metadata: Dict         # 报告元数据
```

**状态流转：**
```
初始状态 → 澄清状态 → 背景调查状态 → 计划状态
→ 审核状态 → 研究状态 → 报告状态 → 完成状态
```

**检查点机制：**
- 每个阶段完成后保存检查点
- 支持中断恢复
- 可配置持久化后端（内存/MongoDB/PostgreSQL）

---

#### 4.1.4 错误处理和重试

**错误类型：**
1. **API 调用失败** - 搜索 API、LLM API
2. **工具执行失败** - 爬虫超时、代码执行错误
3. **计划验证失败** - 步骤不合理、资源不足
4. **报告生成失败** - 格式错误、内容不完整

**重试策略：**
```python
# 指数退避重试
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def call_llm_with_retry(prompt):
    return llm.invoke(prompt)
```

**降级策略：**
- LLM 失败 → 切换到备用模型
- 搜索失败 → 使用备用搜索引擎
- 爬虫失败 → 跳过该步骤，继续执行

### 4.2 多智能体协作

**智能体分工：**

| 智能体 | 工具 | 典型任务 |
|--------|------|----------|
| **Researcher** | 搜索、爬虫、RAG | 查找资料、提取内容 |
| **Coder** | Python REPL | 数据分析、可视化、计算 |
| **Analyst** | 无特定工具 | 信息综合、逻辑推理 |
| **Reporter** | 无特定工具 | 报告撰写、格式化 |

**协作机制：**
- 基于 LangGraph 的状态共享
- 消息传递系统
- 条件路由（根据计划步骤类型分配智能体）

### 4.3 工具集成

**搜索引擎：**
- Tavily（AI 优化搜索）
- InfoQuest（字节跳动 BytePlus）
- DuckDuckGo（隐私搜索）
- Brave Search
- Arxiv（学术论文）

**爬虫工具：**
- Jina Reader（免费）
- InfoQuest Crawler（高级）

**RAG 知识库：**
- Milvus
- Qdrant
- RAGFlow
- VikingDB

**其他工具：**
- Python REPL（代码执行）
- Volcengine TTS（语音合成）
- MCP 服务器（扩展能力）

---

## 五、项目特色功能

### 5.1 人机协作机制

**多轮澄清：**
- 自动检测模糊的研究主题
- 通过对话明确用户意图
- 提高研究精准度，减少无效搜索

**计划审核：**
- 生成计划后暂停等待用户反馈
- 支持自然语言修改计划
- 可配置自动接受模式

### 5.2 内容生成能力

**支持的内容类型：**
1. **研究报告** - Markdown 格式，带引用
2. **播客脚本** - 对话式内容，可转语音
3. **PPT 演示** - 基于 Marp 生成
4. **散文内容** - 创意写作

### 5.3 MCP 集成

**什么是 MCP？**
Model Context Protocol - 统一的 AI 工具协议

**支持的 MCP 服务：**
- GitHub Trending
- 文件系统访问
- 数据库查询
- 自定义工具

**配置方式：**
在 `conf.yaml` 中添加 MCP 服务器配置。

---

## 六、学习路径建议

### 6.1 初学者路径（1-2天）

**第一步：运行示例**
```bash
# 使用命令行模式快速体验
uv run main.py --interactive
# 选择内置问题，观察执行流程
```

**第二步：理解架构**
- 阅读 `README.md` 了解整体架构
- 查看 `src/graph/builder.py` 理解工作流构建
- 查看 `src/agents/agents.py` 理解智能体定义

**第三步：配置自己的 LLM**
- 修改 `conf.yaml` 配置你的模型
- 尝试不同的 LLM 提供商

### 6.2 进阶路径（3-7天）

**深入工作流：**
- 研究 `src/graph/nodes.py` 中的节点实现
- 理解状态管理机制（`src/graph/types.py`）
- 学习条件路由和消息传递

**工具开发：**
- 查看 `src/tools/` 下的工具实现
- 尝试添加自定义工具
- 集成新的搜索引擎或 API

**前端定制：**
- 研究 `web/src/app/chat/` 的聊天界面
- 修改 UI 组件和样式
- 添加新的功能页面

### 6.3 高级路径（1-2周）

**多智能体系统设计：**
- 添加新的智能体类型
- 设计复杂的协作流程
- 优化智能体分工策略

**性能优化：**
- 实现缓存机制
- 优化 LLM 调用策略
- 并行化研究步骤

**生产部署：**
- 配置检查点持久化
- 添加认证和权限控制
- 监控和日志系统

---

## 七、常见问题

### Q1: 如何选择 LLM 模型？
**A:**
- 快速测试：使用 DeepSeek（性价比高）
- 高质量输出：使用 GPT-4o 或 Claude
- 本地部署：使用 Ollama + Qwen

### Q2: 搜索引擎如何选择？
**A:**
- 推荐：InfoQuest（字节跳动，质量高）
- 免费：DuckDuckGo（无需 API key）
- AI 优化：Tavily（专为 AI 设计）

### Q3: 如何降低成本？
**A:**
- 使用更便宜的模型（DeepSeek、Qwen）
- 减少 `max_step_num` 参数
- 启用缓存机制
- 使用免费搜索引擎

### Q4: 如何提高研究质量？
**A:**
- 启用澄清机制（`enable_clarification=true`）
- 增加计划步骤数（`max_step_num`）
- 使用更强的 LLM 模型
- 集成 RAG 私有知识库

---

## 八、总结

DeerFlow 是一个功能强大的多智能体研究框架，具有以下优势：

✅ **架构清晰** - 基于 LangGraph 的模块化设计
✅ **易于扩展** - 支持自定义智能体和工具
✅ **灵活配置** - 多 LLM、多搜索引擎、多 RAG
✅ **人机协作** - 澄清机制和计划审核
✅ **生产就绪** - 支持检查点、监控、部署

**适合人群：**
- AI 应用开发者
- 研究人员和内容创作者
- 想学习多智能体系统的工程师
- 需要自动化研究工具的团队

**下一步行动：**
1. 克隆项目并运行示例
2. 配置你的 LLM 和搜索 API
3. 尝试修改提示词和工作流
4. 贡献代码或提出改进建议

---

**项目地址：** https://github.com/bytedance/deer-flow
**官方网站：** https://deerflow.tech/
**文档：** [Configuration Guide](https://github.com/bytedance/deer-flow/blob/main/docs/configuration_guide.md)
